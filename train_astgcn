from tqdm import tqdm

import torch
import torch.nn.functional as F
from torch_geometric_temporal.nn.recurrent import A3TGCN
import numpy as np
import networkx as nx
from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader
from torch_geometric_temporal.signal import temporal_signal_split
from torch_geometric.data import Data
from torch_geometric.utils import barabasi_albert_graph
from torch_geometric.transforms import LaplacianLambdaMax
from torch_geometric_temporal.nn.attention import (
    TemporalConv,
    STConv,
    ASTGCN,
    MSTGCN,
    MTGNN,
    ChebConvAttention,
    AAGCN,
    GraphAAGCN,
    DNNTSP,
)
from torch_geometric_temporal.nn.attention import (
    GMAN,
    SpatioTemporalAttention,
    SpatioTemporalEmbedding,
)


from utils.dataloadn1 import get_YlabelDataSet,get_XnodeFeaDataSet,get_edgeFeaDataSet


print('letm s go')
from torch_geometric_temporal.nn.attention import (
    TemporalConv,
    STConv,
    ASTGCN,
    MSTGCN,
    MTGNN,
    ChebConvAttention,
    AAGCN,
    GraphAAGCN,
    DNNTSP,
)


'''
loader = ChickenpoxDatasetLoader()

dataset = loader.get_dataset()

train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)

class RecurrentGCN(torch.nn.Module):
    def __init__(self, node_features, periods):
        super(RecurrentGCN, self).__init__()
        self.recurrent = A3TGCN(node_features, 32, periods)
        self.linear = torch.nn.Linear(32, 1)

    def forward(self, x, edge_index, edge_weight):
        h = self.recurrent(x.view(x.shape[0], 1, x.shape[1]), edge_index, edge_weight)
        h = F.relu(h)
        h = self.linear(h)
        return h
        
model = RecurrentGCN(node_features = 1, periods = 4)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.train()

for epoch in tqdm(range(50)):
    cost = 0
    for time, snapshot in enumerate(train_dataset):
        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
        cost = cost + torch.mean((y_hat-snapshot.y)**2)
    cost = cost / (time+1)
    cost.backward()
    optimizer.step()
    optimizer.zero_grad()
    
model.eval()
cost = 0
for time, snapshot in enumerate(test_dataset):
    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat-snapshot.y)**2)
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
'''



def create_mock_data(number_of_nodes, edge_per_node, in_channels):
    """
    Creating a mock feature matrix and edge index.
    """
    graph = nx.watts_strogatz_graph(number_of_nodes, edge_per_node, 0.5)
    edge_index = torch.LongTensor(np.array([edge for edge in graph.edges()]).T)
    X = torch.FloatTensor(np.random.uniform(-1, 1, (number_of_nodes, in_channels)))
    return X, edge_index


def create_mock_edge_weight(edge_index):
    """
    Creating a mock edge weight tensor.
    """
    return torch.FloatTensor(np.random.uniform(0, 1, (edge_index.shape[1])))


def create_mock_target(number_of_nodes, number_of_classes):
    """
    Creating a mock target vector.
    """
    return torch.LongTensor(
        [np.random.randint(0, number_of_classes - 1) for node in range(number_of_nodes)]
    )


def create_mock_sequence(
    sequence_length, number_of_nodes, edge_per_node, in_channels, number_of_classes
):
    """
    Creating mock sequence data

    Note that this is a static graph discrete signal type sequence
    The target is the "next" item in the sequence
    """
    input_sequence = torch.zeros(sequence_length, number_of_nodes, in_channels)

    X, edge_index = create_mock_data(
        number_of_nodes=number_of_nodes,
        edge_per_node=edge_per_node,
        in_channels=in_channels,
    )
    edge_weight = create_mock_edge_weight(edge_index)
    targets = create_mock_target(number_of_nodes, number_of_classes)

    for t in range(sequence_length):
        input_sequence[t] = X + t

    return input_sequence, targets, edge_index, edge_weight


def create_mock_batch(
    batch_size,
    sequence_length,
    number_of_nodes,
    edge_per_node,
    in_channels,
    number_of_classes,
):
    """
    Creating a mock batch of sequences
    """
    batch = torch.zeros(batch_size, sequence_length, number_of_nodes, in_channels)
    batch_targets = torch.zeros(batch_size, number_of_nodes, dtype=torch.long)

    for b in range(batch_size):
        input_sequence, targets, edge_index, edge_weight = create_mock_sequence(
            sequence_length,
            number_of_nodes,
            edge_per_node,
            in_channels,
            number_of_classes,
        )
        batch[b] = input_sequence
        batch_targets[b] = targets

    return batch, batch_targets, edge_index, edge_weight







def train_astgcn():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    in_channels, out_channels = (16, 32)
    batch_size = 3
    edge_index = torch.tensor([[0, 0, 0, 1, 2, 3], [1, 2, 3, 0, 0, 0]]).to(device)
    num_nodes = edge_index.max().item() + 1
    edge_weight = torch.rand(edge_index.size(1)).to(device)
    x = torch.randn((batch_size, num_nodes, in_channels)).to(device)
    attention = torch.nn.functional.softmax(
        torch.rand((batch_size, num_nodes, num_nodes)), dim=1
    ).to(device)
    print('begin run')
    conv = ChebConvAttention(in_channels, out_channels, K=3, normalization="sym").to(
        device
    )
    assert conv.__repr__() == "ChebConvAttention(16, 32, K=3, normalization=sym)"
    out1 = conv(x, edge_index, attention)
    assert out1.size() == (batch_size, num_nodes, out_channels)
    out2 = conv(x, edge_index, attention, edge_weight)
    assert out2.size() == (batch_size, num_nodes, out_channels)
    out3 = conv(x, edge_index, attention, edge_weight, lambda_max=3.0)
    assert out3.size() == (batch_size, num_nodes, out_channels)

    batch = torch.tensor([0, 0, 1, 1]).to(device)
    edge_index = torch.tensor([[0, 1, 2, 3], [1, 0, 3, 2]]).to(device)
    num_nodes = edge_index.max().item() + 1
    edge_weight = torch.rand(edge_index.size(1)).to(device)
    x = torch.randn((batch_size, num_nodes, in_channels)).to(device)
    lambda_max = torch.tensor([2.0, 3.0]).to(device)
    attention = torch.nn.functional.softmax(
        torch.rand((batch_size, num_nodes, num_nodes)), dim=1
    ).to(device)

    out4 = conv(x, edge_index, attention, edge_weight, batch)
    assert out4.size() == (batch_size, num_nodes, out_channels)
    out5 = conv(x, edge_index, attention, edge_weight, batch, lambda_max)
    assert out5.size() == (batch_size, num_nodes, out_channels)

    node_count = 307
    num_classes = 10
    edge_per_node = 15

    num_for_predict = 12
    len_input = 12
    nb_time_strides = 1

    node_features = 2
    nb_block = 2
    K = 3
    nb_chev_filter = 64
    nb_time_filter = 64
    batch_size = 32
    batch_num = 90
    normalization = None
    bias = True
    model = ASTGCN(
        nb_block,
        node_features,
        K,
        nb_chev_filter,
        nb_time_filter,
        nb_time_strides,
        num_for_predict,
        len_input,
        node_count,
        normalization,
        bias,
    ).to(device)
    T = len_input
    x_seq = torch.zeros([batch_size*batch_num, node_count, node_features, T]).to(device)
    target_seq = torch.zeros([batch_size*batch_num, node_count, T]).to(device)
    edge_index_seq = []
    for num in range(batch_num):
        for b in range(batch_size):
            for t in range(T):
                x, edge_index = create_mock_data(node_count, edge_per_node, node_features)
                x_seq[b+num*batch_size, :, :, t] = x.to(device)
                if b == 0:
                    edge_index_seq.append(edge_index.to(device))
                target = create_mock_target(node_count, num_classes).to(device)
                target_seq[b+num*batch_size, :, t] = target
    shuffle = True
    train_dataset = torch.utils.data.TensorDataset(x_seq, target_seq)

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size, shuffle=shuffle
    )
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    model.train()
    for batch_data in train_loader:
        encoder_inputs, target_y = batch_data
        outputs0 = model(encoder_inputs, edge_index_seq)    
        loss = torch.mean((outputs0-target_y)**2)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        print("MSE: {:.4f}".format(loss))

if __name__ == '__main__':
    train_astgcn()

